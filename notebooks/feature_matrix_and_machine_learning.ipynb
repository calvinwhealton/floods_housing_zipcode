{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Matrix and Machine Learning\n",
    "_Calvin Whealton_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook combines the cleaned variables into a feature matrix. The features in the matrix include the population density, median income, US GDP (quarterly), and time series of month-over-month change in Zillow Housing Value Index (ZHVI). This feature matrix will be used in the machine learning portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from scipy.stats import norm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets for Zip Codes, Housing Data, and Peak Flows\n",
    "\n",
    "This section loads many of the input files. Initially, only the zip codes with flood and housing data are considered as viable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip Code Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shapefile is too large to be uploaded to github\n",
    "#shapefile available from https://drive.google.com/file/d/1yTwgTfbYZirtNQOIfgQVDY4Tc-QKDVTb/view?usp=sharing\n",
    "os.chdir('/Users/calvinwhealton/Documents/GitHub/floods_housing_zipcode/data/geo_data/tl_2019_us_zcta510_clipped48contig')\n",
    "zips_shapefile = gpd.read_file('clipped48contig.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips_shapefile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips_shapefile.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housing Data\n",
    "\n",
    "Zillow Housing Value Index (ZHVI) that was processed to be a month-over-month percentage change. Values are indexed by the last day of the month. Only the year-month will be used when referencing the time later in the combining of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/calvinwhealton/Documents/GitHub/floods_housing_zipcode/data/processed_data')\n",
    "housing = pd.read_csv('zillow_mon_pct_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stripping day from the column names\n",
    "for i in housing.columns[2:295]:\n",
    "    housing = housing.rename(columns={i: i[0:7]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### National Flood Insurance Claims Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/calvinwhealton/Documents/GitHub/floods_housing_zipcode/data/processed_data')\n",
    "claims = pd.read_csv('ts_claims_month.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flood Gages\n",
    "\n",
    "Time series of return periods of floods. Return period is inverse of exceedance probability of the flood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/calvinwhealton/Documents/GitHub/floods_housing_zipcode/data/processed_data')\n",
    "ret_pers = pd.read_csv('ts_rps_2020-08-15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/calvinwhealton/Documents/GitHub/floods_housing_zipcode/data/processed_data')\n",
    "zip_gage = pd.read_csv('zip_gage_dist_2020-08-10.csv')\n",
    "zip_gage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Finding Zip Codes with Floods\n",
    "\n",
    "Loop through each zip code. Floods with return periods greater than 50 years are identified for gages associated with those zip codes. Return periods are assigned based on the log normal distribution fit with log-space method of moments.\n",
    "\n",
    "Once the floods are identified for each gage, the mean day of flood for each year is determined across the gages. This is taken as the date of the flood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml = pd.DataFrame(columns=['zip','year','month',\n",
    "                                'flood_rp', # population density\n",
    "                                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the set that has all information of interest\n",
    "zips_with_shape_housing = set.intersection(set(zips_shapefile['GEOID10'].astype(int).values), set(housing['GEOID10_str'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used in extracting gage numbers\n",
    "gage_num_cols = ['gage0','gage1','gage2','gage3','gage4','gage5','gage6','gage7','gage8','gage9']\n",
    "\n",
    "# loop over zip codes\n",
    "for z in list(zips_with_shape_housing):\n",
    "    \n",
    "    # set of gages pre-processed as relevant for zip code\n",
    "    gages_for_zip = zip_gage.loc[zip_gage['GEOID10']==int(z),gage_num_cols]\n",
    "    \n",
    "    dates_check = []\n",
    "    rps_check = []\n",
    "    \n",
    "    # loop over gages for the zip code\n",
    "    for g in gages_for_zip.iloc[0].values:\n",
    "        \n",
    "        # reading in the file for the gage\n",
    "        # sometimes need to pad with opening 0s\n",
    "        os.chdir('/Users/calvinwhealton/Documents/GitHub/floods_housing_zipcode/data/gage_data/peak_flows')\n",
    "        if str(g).zfill(8)+'.csv' in os.listdir('/Users/calvinwhealton/Documents/GitHub/floods_housing_zipcode/data/gage_data/peak_flows'):\n",
    "            \n",
    "            # pad the gage number to account for leading 0s\n",
    "            gage_name = str(g).zfill(8)+'.csv'\n",
    "            gage_data = pd.read_csv(gage_name, comment='#')\n",
    "            \n",
    "            # get peak flows and dates\n",
    "            peaks = gage_data['peak_va'].values\n",
    "            dates = gage_data['peak_dt'].values\n",
    "            \n",
    "            # drop nan values (missing flood record)\n",
    "            keepers = (np.isnan(peaks) == False)\n",
    "            \n",
    "            peaks = peaks[keepers]\n",
    "            dates = dates[keepers]\n",
    "            \n",
    "            # log-space mean and variance for log-normal distribution\n",
    "            ls_mean = np.mean(np.log(peaks))\n",
    "            ls_sd = np.std(np.log(peaks))\n",
    "            \n",
    "            # calculation of return period, rp(x) = 1/(1- cdf(x))\n",
    "            rps = 1/(1-norm.cdf(np.log(peaks),ls_mean,ls_sd))\n",
    "            \n",
    "            # because interested in extremes, dropping floods below 50-yr return period (2% exceedance)\n",
    "            keep_extremes = (rps > 50)\n",
    "            rps_extremes = rps[keep_extremes]\n",
    "            dates_extremes = dates[keep_extremes]\n",
    "\n",
    "            if len(rps_extremes) > 0:\n",
    "                \n",
    "                # imputing a month-day or day when it is 00\n",
    "                # 00 would indicate that the value is either fairly old (e.g. 1863-00-00)\n",
    "                # or that the gage does not automatrically record values\n",
    "                for j in range(len(dates_extremes)):\n",
    "                    dates_extremes[j] = dates_extremes[j].replace('-00-00','-06-15')\n",
    "                    dates_extremes[j] = dates_extremes[j].replace('-00','-15')\n",
    "                \n",
    "                # convert to date-time index\n",
    "                # extract the appropriate dates and return periods\n",
    "                datetime_extremes = pd.DatetimeIndex(pd.Series(dates_extremes))\n",
    "                extremes_use = np.where(np.logical_and(datetime_extremes.year > 1996, datetime_extremes.year < 2019))\n",
    "                dates_use = datetime_extremes[extremes_use]\n",
    "                rps_use = rps_extremes[extremes_use]\n",
    "                \n",
    "                # appending to list of dates and return periods for the zip code\n",
    "                if len(rps_use) > 0:\n",
    "                    dates_check.extend(dates_use)\n",
    "                    rps_check.extend(rps_use)\n",
    "        \n",
    "        # identifying dates and typical return periods\n",
    "        temp_dates = pd.DataFrame({'date': dates_check, 'rps': rps_check})\n",
    "        temp_dates['log_rp'] = np.log(temp_dates['rps'])\n",
    "        temp_dates['year'] = pd.DatetimeIndex(temp_dates['date']).year\n",
    "        \n",
    "        # identified_floods (one per calendar year)\n",
    "        flood_dates = temp_dates.groupby('year')['date'].agg(lambda x: x.mean())\n",
    "        flood_rps = np.exp(temp_dates.groupby('year')['log_rp'].agg(lambda x: x.mean()))\n",
    "    \n",
    "    # add the mean day of flood and geometric mean of return period to the data frame\n",
    "    # reference information includes the zip code, year, and month\n",
    "    for c in range(len(flood_dates)):\n",
    "        data_ml = data_ml.append({'zip': z,\n",
    "                                     'year': pd.DatetimeIndex(flood_dates).year.values[c],\n",
    "                                     'month': pd.DatetimeIndex(flood_dates).month.values[c],\n",
    "                                     'flood_rp': flood_rps.values[c]},ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to make sure it all looks okay\n",
    "data_ml.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml.to_csv('data_ml_after_floods.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Adding Housing Value\n",
    "\n",
    "The mean day of the flood (year-month) are used to find the accompanying housing price data for the zip code. Months between 12 before and 12 after the flood are extracted. There is processing to handle NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used in naming columns\n",
    "housing_price_cols = ['h-12','h-11', 'h-10','h-09','h-08','h-07','h-06','h-05','h-04','h-03','h-02','h-01','h+00','h+01','h+02','h+03','h+04','h+05','h+06','h+07','h+08','h+09','h+10','h+11','h+12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(housing_price_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading-in data already calculated\n",
    "data_ml = pd.read_csv('data_ml_after_floods.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing housing price data in the machine learning dataframe\n",
    "for h in housing_price_cols:\n",
    "    data_ml[h] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to extract the housing data and paste it in to the machine learning dataframe\n",
    "# take the year-month of the mean day of flood for the year and use that as the zero index\n",
    "# take 12 months before and after that day\n",
    "for ind in data_ml.index:\n",
    "    yr_check = int(data_ml.loc[ind,'year'])\n",
    "    mo_check = int(data_ml.loc[ind,'month'])\n",
    "    \n",
    "    date_list = [str(yr_check) + '-' + str(mo_check).zfill(2)]\n",
    "    for i in range(1,13):\n",
    "        if mo_check - i <= 0:\n",
    "            date_list.insert(0, str(yr_check-1) + '-' + str(mo_check - i+12).zfill(2))\n",
    "        else:\n",
    "            date_list.insert(0, str(yr_check) + '-' + str(mo_check - i).zfill(2))\n",
    "    \n",
    "    for i in range(1,13):\n",
    "        if mo_check + i >= 12:\n",
    "            date_list.append(str(yr_check+1) + '-' + str(mo_check + i-12).zfill(2))\n",
    "        else:\n",
    "            date_list.append(str(yr_check) + '-' + str(mo_check + i).zfill(2))\n",
    "    \n",
    "    # extracting housing price data\n",
    "    housing_temp = housing.loc[housing['GEOID10_str'].values==int(data_ml.loc[ind,'zip']), date_list]\n",
    "    housing_use = housing_temp.iloc[0].values\n",
    "         \n",
    "    data_ml.loc[ind,housing_price_cols] = housing_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that it looks okay\n",
    "data_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns with a lot of nan values\n",
    "# indicate large gaps in the housing price data for that zip code over the time period\n",
    "data_ml = data_ml.loc[data_ml.isnull().sum(axis=1) <= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml.to_csv('data_ml_with_housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml = pd.read_csv('data_ml_with_housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up zip codes with one missing (NaN value)\n",
    "# when it is between two other values, the value is linearly interpolated\n",
    "# when it is on the edge, the value from the neighbor is used\n",
    "for i in data_ml.index:\n",
    "    if data_ml.loc[i].isnull().sum() == 1:\n",
    "        if np.isnan(data_ml.loc[i,'h-11']):\n",
    "            data_ml.loc[i,'h-11'] = data_ml.loc[i,'h-10']\n",
    "        elif np.isnan(data_ml.loc[i,'h+12']):\n",
    "            data_ml.loc[i,'h+12'] = data_ml.loc[i,'h+11']\n",
    "        else:\n",
    "            for h in range(len(housing_price_cols)):\n",
    "                if np.isnan(data_ml.loc[i,housing_price_cols[h]]):\n",
    "                    data_ml.loc[i,housing_price_cols[h]] = 0.5*(data_ml.loc[i,housing_price_cols[h+1]] + data_ml.loc[i,housing_price_cols[h-1]])\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying that no NaNs remain\n",
    "data_ml.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml.to_csv('data_ml_with_housing_imputedNaN.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: GDP Information\n",
    "\n",
    "GPD is taken as representative of the overall national economy. Disasters in a recession might not be the same as disasters during a boom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml = pd.read_csv('data_ml_with_housing_imputedNaN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/calvinwhealton/Documents/GitHub/floods_housing_zipcode/data')\n",
    "gdp = pd.read_csv('A191RL1Q225SBEA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp['datetime'] = pd.DatetimeIndex(gdp['DATE'])\n",
    "gdp['year'] = pd.DatetimeIndex(gdp['DATE']).year\n",
    "gdp['month'] = pd.DatetimeIndex(gdp['DATE']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml['GDP'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to obtain the gdps\n",
    "for i in data_ml.index:\n",
    "    # make a datetime object from flood year-month\n",
    "    d = datetime.datetime(int(data_ml.loc[i,'year']), int(data_ml.loc[i,'month']), 1)\n",
    "    \n",
    "    # extract the first GDP that is greater than the time index of the flood\n",
    "    # will put it in the right quarter\n",
    "    data_ml.loc[i,'GDP'] = gdp.loc[gdp['datetime'] >= d,'A191RL1Q225SBEA'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/calvinwhealton/Documents/GitHub/floods_housing_zipcode/data/processed_data')\n",
    "data_ml.to_csv('data_ml_clean_housing_gpd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Demographic Data\n",
    "\n",
    "Appending the median household income and population density for the zip code. These are the last two features considered in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/calvinwhealton/Documents/GitHub/floods_housing_zipcode/data/processed_data')\n",
    "data_ml = pd.read_csv('data_ml_clean_housing_gpd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/calvinwhealton/Documents/GitHub/floods_housing_zipcode/data')\n",
    "zcta_cousub = pd.read_csv('zcta_countysub_uscensus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcta_cousub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe with the minimal data needed\n",
    "zips_key_vals = pd.DataFrame({'zips':zips_shapefile['GEOID10'].astype(int).values,\n",
    "                              'area':zips_shapefile['ALAND10'].values })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips_key_vals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate dataframe for population\n",
    "pop_df = pd.DataFrame({'zips':(zcta_cousub.groupby('ZCTA5').mean())['ZPOP'].index,\n",
    "                      'zpop':(zcta_cousub.groupby('ZCTA5').mean())['ZPOP']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging values in a series of operations\n",
    "zips_key_vals2 = pd.merge(left=zips_key_vals, right = pop_df, left_on = 'zips', right_on = 'zips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips_key_vals2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips_key_vals2['pop_dens'] = zips_key_vals2['zpop']/zips_key_vals2['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/calvinwhealton/Documents/GitHub/floods_housing_zipcode/data/processed_data')\n",
    "zip_medinc = pd.read_csv('zips_med_inc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_medinc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips_key_vals3 = pd.merge(left=zips_key_vals2, right = zip_medinc, left_on = 'zips', right_on = 'zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips_key_vals3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding data to the machine learning dataframe\n",
    "data_ml['pop_dens'] = np.nan\n",
    "data_ml['med_inc'] = np.nan\n",
    "\n",
    "for i in data_ml.index:\n",
    "    if data_ml.loc[i,'zip'] in zips_key_vals3['zip'].values:\n",
    "        data_ml.loc[i,'pop_dens'] = zips_key_vals3.loc[zips_key_vals3['zips'].values==data_ml.loc[i,'zip'],'pop_dens'].values\n",
    "        data_ml.loc[i,'med_inc'] = zips_key_vals3.loc[zips_key_vals3['zips'].values==data_ml.loc[i,'zip'],'med_hh_inc'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking to make sure all the data looks valid. No input/output problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(data_ml['year'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might not be needed.\n",
    "data_ml.drop(columns=['GPD', 'Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'Unnamed: 0.1.1.1'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep the complete cases\n",
    "# no imputation of missing demographic data\n",
    "data_ml.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/calvinwhealton/Documents/GitHub/floods_housing_zipcode/data/processed_data')\n",
    "data_ml.to_csv('data_ml_flood_hou_gdp_pop.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macine Learning\n",
    "\n",
    "Goal of the Machine Learning part of this project is to predict the typical and range of responses one might expect following a flood for the zip code based on the attributes given above. The selected algorithm is K Nearest Neighbors (KNN), some processing of the features will be required.\n",
    "\n",
    "_Flood Return Period_ : Log-transformed because a 50-year and 200-year flood are in some sense the same distance away from a 100-year flood. They are both a factor of 2. There could be some threshold effects around the 100-year flood, a common value used in designs, but it is difficult to accurately assess the 100-year flood even with 100 years of data.\n",
    "\n",
    "_Median Household Income_ : Log-transformed. The values are spread over roughly an order of magnitude. It is anticipated that the impact of a flood on an area with 20 kUSD/yr and one with 50 kUSD/yr would be more significant than 120 kUSD/yr vs 150 kUSD/yr.\n",
    "\n",
    "_Population Density_ : Log-transformed. The values are roughly spread over 6 orders of magnitude. This variable is a proxy for how urban or rural an area is.\n",
    "\n",
    "_Housing Values_ : The variable has already been transformed in to month-over-month percentage increase in the housing value. This will be further condensed into a single sum of squares difference between the pre-flood time series and the location desired to be estimated.\n",
    "\n",
    "_GDP_ : Scaled based on range of values. Zero still assumed to map to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/calvinwhealton/Documents/GitHub/floods_housing_zipcode/data/processed_data')\n",
    "data_ml = pd.read_csv('data_ml_flood_hou_gdp_pop.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import base\n",
    "class ColumnSelectTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    '''\n",
    "    Transformer used in the practical machine learning mini project\n",
    "    Selects the columns defined as col_names from the dataframe\n",
    "    Returns the values for those columns\n",
    "    Does not need to learn anything about the data\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, col_names):\n",
    "        self.col_names = col_names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        rets = np.zeros((X.shape[0], len(self.col_names)))\n",
    "        for c in range(len(self.col_names)):\n",
    "            rets[:,c] = X[self.col_names[c]]\n",
    "        return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    '''\n",
    "    Transforms columns as the logarithm of the given values\n",
    "    It does not have to learn anything about the data\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.log(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesRescaler(base.BaseEstimator, base.TransformerMixin):\n",
    "    '''\n",
    "    Transforms columns as a time series\n",
    "    scales based on the standard deviation\n",
    "    Does not shift the mean value\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.std = np.std(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [row/self.std for row in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoveRefScale(base.BaseEstimator, base.TransformerMixin):\n",
    "    '''\n",
    "    Transforms columns based on\n",
    "    ref = reference value (shift values with respect to what)\n",
    "    scalter = option for how to measure spread for normalization\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,ref=None,scaler='std'):\n",
    "        self.scaler = scaler\n",
    "        self.ref = ref\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if self.ref is None:\n",
    "            self.ref_use = np.mean(X)\n",
    "        else:\n",
    "            self.ref_use = self.ref\n",
    "        \n",
    "        if self.scaler == 'std':\n",
    "            self.scale_value = np.std(X)\n",
    "        if self.scaler == 'min_max':\n",
    "            self.scale_value = np.max(X) - np.min(X)\n",
    "        if self.scaler == 'iqr':\n",
    "            self.scale_value = np.quantile(X,0.75) - np.quantile(X,0.25)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return (X-self.ref_use)/self.scale_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general manipulation of the data requires the use of pipelines and feature unions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe_rp = Pipeline([\n",
    "                ('cst_rp', ColumnSelectTransformer(col_names=['flood_rp'])),\n",
    "                ('lt_rp', LogTransformer()),\n",
    "                ('mrs_rp',  MoveRefScale(ref=np.log(100), scaler='iqr'))\n",
    "])\n",
    "\n",
    "\n",
    "pipe_gdp = Pipeline([\n",
    "                ('cst_gdp', ColumnSelectTransformer(col_names=['GDP'])),\n",
    "                ('mrs_gdp', MoveRefScale(ref=0.0, scaler='std'))\n",
    "])\n",
    "\n",
    "pipe_inc = Pipeline([\n",
    "                ('cst_inc', ColumnSelectTransformer(col_names=['med_inc'])),\n",
    "                ('lt_inc', LogTransformer()),\n",
    "                ('mrs_inc', MoveRefScale(ref=None,scaler='std'))\n",
    "])\n",
    "\n",
    "pipe_popden = Pipeline([\n",
    "                ('cst_pden',  ColumnSelectTransformer(col_names=['pop_dens'])),\n",
    "                ('lt_pden', LogTransformer()),\n",
    "                ('mrs_pden', MoveRefScale(ref=None,scaler='std'))\n",
    "])\n",
    "\n",
    "pipe_houTS = Pipeline([\n",
    "                ('cst_gdp', ColumnSelectTransformer(col_names=['h-12', 'h-11','h-10','h-09', 'h-08','h-07','h-06','h-05','h-04','h-03','h-02','h-01'])),\n",
    "                ('tsr_gdp', TimeSeriesRescaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ml = data_ml[['flood_rp','GDP','med_inc','pop_dens','h-12', 'h-11','h-10','h-09', 'h-08','h-07','h-06','h-05','h-04','h-03','h-02','h-01']].copy()\n",
    "y_ml = data_ml[['h+00','h+01','h+02','h+03','h+04','h+05','h+06','h+07','h+08','h+09','h+10','h+11','h+12']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union = FeatureUnion([\n",
    "        ('rp',pipe_rp),\n",
    "        ('gdp',pipe_gdp),\n",
    "        ('inc',pipe_inc),\n",
    "        ('popden',pipe_popden),\n",
    "        ('houTS', pipe_houTS)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union.fit(X_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNMixedTSConsts(base.BaseEstimator, base.RegressorMixin):\n",
    "    '''\n",
    "    Custom estimator for the time series data (and non-time series) for problem\n",
    "    neighbors = number of neighbors\n",
    "    ts_inds = indices of the time series (assumed to be in correct order)\n",
    "    weights = weights for different parts of distance (time series collapsed to singe distance then weighted)\n",
    "    '''\n",
    "\n",
    "    def __init__(self,neighbors,ts_inds, weights):\n",
    "        self.neighbors = neighbors\n",
    "        self.ts_inds = ts_inds\n",
    "        self.weights = weights\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X = X # store the values passed in\n",
    "        self.y = y\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # prediction will be the mean of the k nearest neighbors\n",
    "        # prediction also will return 80% interval\n",
    "        # size will be number_of_prediction * length_of_time_series * number_of_metrics\n",
    "        num_metrics = 3\n",
    "        num_preds = X.shape[0]\n",
    "        length_of_ts = self.y.shape[1]\n",
    "        \n",
    "        pred_arr = np.zeros((num_preds, length_of_ts, num_metrics))\n",
    "        \n",
    "        ts_vals = np.array(self.y)\n",
    "        \n",
    "        for p in range(num_preds):\n",
    "            \n",
    "            # calculate the distance\n",
    "            dists = dist_calc(X[p,:], self.X, self.ts_inds, self.weights)\n",
    "            \n",
    "            # find neighbors by index\n",
    "            neighbors_close = (np.argsort(dists))[0:self.neighbors]\n",
    "            \n",
    "            # take mean down the columns\n",
    "            # length will be same as number of columns\n",
    "            # also estimate the quantiles\n",
    "            pred_arr[p,:,0] = np.mean(ts_vals[neighbors_close],axis=0)\n",
    "            \n",
    "            pred_arr[p,:,1] = np.quantile(ts_vals[neighbors_close], 0.1,axis=0)\n",
    "            pred_arr[p,:,2] = np.quantile(ts_vals[neighbors_close], 0.9,axis=0)\n",
    "        \n",
    "        return pred_arr\n",
    "    \n",
    "    \n",
    "def dist_calc(X_fitting, X_mat, ts_inds, weights):\n",
    "    \n",
    "    # dimensions and initializing an array to store results\n",
    "    nrows = np.array(X_mat).shape[0]\n",
    "    ncols = np.array(X_mat).shape[1] - len(ts_inds) + 1\n",
    "    dist = np.zeros((nrows, ncols))\n",
    "    \n",
    "    # calculate the distances\n",
    "    for i in np.arange(ncols):\n",
    "        if i != (ncols - 1):\n",
    "            dist[:,i] = weights[i]*((np.array(X_mat[:,i])-X_fitting[i])**2)\n",
    "        else:\n",
    "            dist_ts = np.zeros((nrows,len(ts_inds)))\n",
    "            for j in range(len(ts_inds)):\n",
    "                dist_ts[:,j] = ((np.array(X_mat[:,ts_inds[j]])-X_fitting[ts_inds[j]])**2)\n",
    "            dist[:,i] = weights[i]*np.sum(dist_ts,axis=1)\n",
    "    \n",
    "    # return the mean across a row\n",
    "    # will be length equal to number of rows\n",
    "    return np.mean(dist,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe = Pipeline([\n",
    "                    ('union_feature', union),\n",
    "                    ('mix_knn', KNNMixedTSConsts(neighbors=50, ts_inds = np.arange(4,16), weights = [1., 1., 1., 1., 0.1]))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking to make sure things work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe.fit(X_ml,y_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe.predict(X_ml.iloc[0:100,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a version of random search to try and find the optimal parameters. The hyper parameters that can be optimized are:\n",
    "\n",
    "_number of neighbors_ : The number of neighbors to choose. Somewhat arbitary. Probably the minimum value should be 5 but the maximum could be over 100. Setting the range to be 5-200 and sampling on a logarithmic range.\n",
    "\n",
    "_weights for features_ : Weights only need to be relative to one feature, which is chosen to be the flood return period. The weights are essentially accounting for the usefulness of the distance in the other feature dimensions, which is also somewhat depends on the transformations used to obtain those other features. As a safe range, values between 0.01 and 100 will be sampled on a log-scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling data and splitting in to train and test arrays\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_ml_sh, y_ml_sh = shuffle(X_ml, y_ml, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ml_sh, y_ml_sh, test_size=0.35, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting random values for hyper parameters\n",
    "# lots of use of the seed to try and have reproducible results\n",
    "num_hypers_test = 300\n",
    "\n",
    "np.random.seed(seed=3)\n",
    "num_neighbors = np.around(np.exp(np.random.uniform(np.log(5), np.log(200), num_hypers_test))).astype(int)\n",
    "np.random.seed(seed=4)\n",
    "weight_gdp = np.exp(np.random.uniform(np.log(0.01), np.log(100),num_hypers_test))\n",
    "np.random.seed(seed=5)\n",
    "weight_medinc = np.exp(np.random.uniform(np.log(0.01), np.log(100),num_hypers_test))\n",
    "np.random.seed(seed=6)\n",
    "weight_popdens = np.exp(np.random.uniform(np.log(0.01), np.log(100),num_hypers_test))\n",
    "np.random.seed(seed=7)\n",
    "weight_houts = np.exp(np.random.uniform(np.log(0.01), np.log(100),num_hypers_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mse_knn_mix(predictions, reality):\n",
    "    '''\n",
    "    function that calculates the mse and accuracy of the probability interval\n",
    "    takes in a set of predictions and each prediction is paired with a real value\n",
    "    '''\n",
    "    mean_preds = predictions[:,:,0] # index for the mean, 1 = 10%, 2 = 90% estimates\n",
    "    \n",
    "    # evaluating if 80% interval covers 80% of outcomes\n",
    "    interval_accuracy = 0\n",
    "    \n",
    "    for i in range(predictions.shape[0]):\n",
    "        for j in range(predictions.shape[1]):\n",
    "            if np.array(reality)[i,j] >= predictions[i,j,2] or np.array(reality)[i,j] <= predictions[i,j,1]:\n",
    "                interval_accuracy += 1\n",
    "    int_acc = interval_accuracy/(predictions.shape[0]*predictions.shape[1])\n",
    "    \n",
    "    return np.mean(np.mean((mean_preds - np.array(reality))**2)), int_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining pipeline and feature untion before loop testing hyper parameters\n",
    "\n",
    "pipe_rp = Pipeline([\n",
    "            ('cst_rp', ColumnSelectTransformer(col_names=['flood_rp'])),\n",
    "            ('lt_rp', LogTransformer()),\n",
    "            ('mrs_rp',  MoveRefScale(ref=np.log(100), scaler='iqr'))\n",
    "    ])\n",
    "\n",
    "pipe_gdp = Pipeline([\n",
    "            ('cst_gdp', ColumnSelectTransformer(col_names=['GDP'])),\n",
    "            ('mrs_gdp', MoveRefScale(ref=0.0, scaler='std'))\n",
    "    ])\n",
    "\n",
    "pipe_inc = Pipeline([\n",
    "            ('cst_inc', ColumnSelectTransformer(col_names=['med_inc'])),\n",
    "            ('lt_inc', LogTransformer()),\n",
    "            ('mrs_inc', MoveRefScale(ref=None,scaler='std'))\n",
    "    ])\n",
    "\n",
    "pipe_popden = Pipeline([\n",
    "            ('cst_pden',  ColumnSelectTransformer(col_names=['pop_dens'])),\n",
    "            ('lt_pden', LogTransformer()),\n",
    "            ('mrs_pden', MoveRefScale(ref=None,scaler='std'))\n",
    "    ])\n",
    "\n",
    "pipe_houTS = Pipeline([\n",
    "            ('cst_gdp', ColumnSelectTransformer(col_names=['h-12', 'h-11','h-10','h-09', 'h-08','h-07','h-06','h-05','h-04','h-03','h-02','h-01'])),\n",
    "            ('tsr_gdp', TimeSeriesRescaler())\n",
    "    ])\n",
    "\n",
    "# feature union\n",
    "union = FeatureUnion([\n",
    "    ('rp',pipe_rp),\n",
    "    ('gdp',pipe_gdp),\n",
    "    ('inc',pipe_inc),\n",
    "    ('popden',pipe_popden),\n",
    "    ('houTS', pipe_houTS)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell to test all the hyper parameter values\n",
    "%%time\n",
    "mse_acc_knn_mix = np.zeros((num_hypers_test, 2))\n",
    "\n",
    "for i in range(num_hypers_test):\n",
    "    \n",
    "    hyper_loop = [num_neighbors[i], 1, weight_gdp[i], weight_medinc[i], weight_popdens[i], weight_houts[i]]\n",
    "    \n",
    "    # set up pipeline from feature union with knn mixed estimator\n",
    "    knn_pipe = Pipeline([\n",
    "                    ('union_feature', union),\n",
    "                    ('mix_knn', KNNMixedTSConsts(neighbors=hyper_loop[0], ts_inds = np.arange(4,16), weights = hyper_loop[1:len(hyper_loop)]))\n",
    "        ])\n",
    "    \n",
    "    # fitting the pipeline\n",
    "    knn_pipe.fit(X_train,y_train)\n",
    "    \n",
    "    # predicting results\n",
    "    knn_mix_preds = knn_pipe.predict(X_test)\n",
    "    \n",
    "    # store error metrics\n",
    "    mse_acc_knn_mix[i,0], mse_acc_knn_mix[i,1] = calc_mse_knn_mix(knn_mix_preds, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(mse_acc_knn_mix[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_config_ind = np.argsort(mse_acc_knn_mix[:,0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params_opt = [num_neighbors[min_config_ind], 1.0, weight_gdp[min_config_ind], weight_medinc[min_config_ind], weight_popdens[min_config_ind], weight_houts[min_config_ind]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that the weights on the GDP is very high relative to the others. This could suggest that the response to the natural disasters are largely controlled by the overall national economic environment. The median income of the zip code has a very low weight and it is on par with the weight for the time series. However, the time series component on average is about 10 times as large because it is the sum of 13 terms, so the time series and the population density have a similar effective weight.\n",
    "\n",
    "The number of neighbors is relatively small at 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a series of plots to show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_neighbors, y=mse_acc_knn_mix[:,0])\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=weight_gdp, y=mse_acc_knn_mix[:,0])\n",
    "plt.xlabel('GDP Weight')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=weight_medinc, y=mse_acc_knn_mix[:,0])\n",
    "plt.xlabel('Median Income Weight')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=weight_popdens, y=mse_acc_knn_mix[:,0])\n",
    "plt.xlabel('Population Density Weight')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=weight_houts, y=mse_acc_knn_mix[:,0])\n",
    "plt.xlabel('Pre-Flood Housing Weight')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the final model for pickling\n",
    "knn_pipe_opt = Pipeline([\n",
    "                    ('union_feature', union),\n",
    "                    ('mix_knn', KNNMixedTSConsts(neighbors=hyper_params_opt[0], ts_inds = np.arange(4,16), weights = hyper_params_opt[1:len(hyper_loop)]))\n",
    "        ])\n",
    "\n",
    "knn_pipe_opt.fit(X_ml_sh,y_ml_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_preds = knn_pipe_opt.predict(X_ml_sh)\n",
    "calc_mse_knn_mix(opt_preds, y_ml_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/calvinwhealton/Documents/GitHub/floods_housing_zipcode/pickled_models')\n",
    "filename = 'mix_knn_opt.sav'\n",
    "pickle.dump(knn_pipe_opt, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models for Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are a couple of other models that would be used for comparison. Essentially, does the nearest neighbors provide any more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing 1: Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model for the prediction time period is simply the mean of the training time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse_avg_mod = 0\n",
    "\n",
    "for i in range(X_ml_sh.shape[0]):\n",
    "    sse_avg_mod += np.sum((np.mean(np.array(X_ml_sh)[i,4:16]) - np.array(y_ml_sh)[i,:])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_avg_mod = sse_avg_mod/(np.array(y_ml_sh).shape[0]*np.array(y_ml_sh).shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_avg_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing 2: Autoregressive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An admittedly poor AR (Auto-Regressive) model. The time series for fitting the model is the same as the pre-flood time series, and based on that the subsequent post-flood series is estiamted. This is performed for each zip code flood series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AR\n",
    "\n",
    "sse_ar_mod = 0\n",
    "\n",
    "for i in range(X_ml_sh.shape[0]):\n",
    "    model_ar = AR(np.array(X_ml_sh)[0,4:16])\n",
    "    model_fitted = model_ar.fit()\n",
    "    preds = model_fitted.predict(\n",
    "                        start=len(np.array(X_ml_sh)[0,4:16]), \n",
    "                        end=len(np.array(X_ml_sh)[0,4:16]) + 12, \n",
    "                        dynamic=False)\n",
    "    \n",
    "    sse_ar_mod += np.sum((preds - np.array(y_ml_sh)[i,:])**2)\n",
    "    \n",
    "mse_ar_mod = sse_ar_mod/(np.array(y_ml_sh).shape[0]*np.array(y_ml_sh).shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_ar_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing 3: KNN with No Flood Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model assumes that floods have nothing to do with the housing market. Therefore, random time periods and zip codes are chosen across the housing price dataest. The other variables, namely the GDP, median income, population density, and housing prices in the 12 months before the prediction were included as predictors. The same series of transformations is applied and a similar hyper-parameter optimization will be performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random sampling of zip codes and time periods implies that some of these might be repeated or there might be some overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revised down to 200000 for the sake of time\n",
    "num_random_zip_date = 200000 #500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling zip code, year, and month randomly\n",
    "# year chosen as 1997 to 2018 (housing data starts in 1996 so 1997 ensures 12-months before)\n",
    "sam_zip = np.random.choice(np.array(list(zips_with_shape_housing)), size=num_random_zip_date, replace=True)\n",
    "sam_yr = np.random.choice(np.arange(1997, 2019), size=num_random_zip_date, replace=True)\n",
    "sam_mo = np.random.choice(np.arange(1,13), size=num_random_zip_date, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml_noFlood = pd.DataFrame({'zip':sam_zip, 'year':sam_yr, 'month':sam_mo})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in housing_price_cols:\n",
    "    data_ml_noFlood[h] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to extract the housing data and paste it in to the machine learning dataframe\n",
    "# take the year-month of the mean day of flood for the year and use that as the zero index\n",
    "# take 12 months before and after that day\n",
    "for ind in data_ml_noFlood.index:\n",
    "    yr_check = int(data_ml_noFlood.loc[ind,'year'])\n",
    "    mo_check = int(data_ml_noFlood.loc[ind,'month'])\n",
    "    \n",
    "    date_list = [str(yr_check) + '-' + str(mo_check).zfill(2)]\n",
    "    for i in range(1,13):\n",
    "        if mo_check - i <= 0:\n",
    "            date_list.insert(0, str(yr_check-1) + '-' + str(mo_check - i+12).zfill(2))\n",
    "        else:\n",
    "            date_list.insert(0, str(yr_check) + '-' + str(mo_check - i).zfill(2))\n",
    "    \n",
    "    for i in range(1,13):\n",
    "        if mo_check + i >= 12:\n",
    "            date_list.append(str(yr_check+1) + '-' + str(mo_check + i-12).zfill(2))\n",
    "        else:\n",
    "            date_list.append(str(yr_check) + '-' + str(mo_check + i).zfill(2))\n",
    "    \n",
    "    # extracting housing price data\n",
    "    housing_temp = housing.loc[housing['GEOID10_str'].values==int(data_ml_noFlood.loc[ind,'zip']), date_list]\n",
    "    housing_use = housing_temp.iloc[0].values\n",
    "         \n",
    "    data_ml_noFlood.loc[ind,housing_price_cols] = housing_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns with a lot of nan values\n",
    "# indicate large gaps in the housing price data for that zip code over the time period\n",
    "data_ml_noFlood = data_ml_noFlood.iloc[0:202426]\n",
    "data_ml_noFlood = data_ml_noFlood.loc[data_ml_noFlood.isnull().sum(axis=1) <= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml_noFlood.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up zip codes with one missing (NaN value)\n",
    "# when it is between two other values, the value is linearly interpolated\n",
    "# when it is on the edge, the value from the neighbor is used\n",
    "for i in data_ml_noFlood.index:\n",
    "    if data_ml_noFlood.loc[i].isnull().sum() == 1:\n",
    "        if np.isnan(data_ml_noFlood.loc[i,'h-11']):\n",
    "            data_ml_noFlood.loc[i,'h-11'] = data_mlFlood.loc[i,'h-10']\n",
    "        elif np.isnan(data_ml_noFlood.loc[i,'h+12']):\n",
    "            data_ml_noFlood.loc[i,'h+12'] = data_ml_noFlood.loc[i,'h+11']\n",
    "        else:\n",
    "            for h in range(len(housing_price_cols)):\n",
    "                if np.isnan(data_ml_noFlood.loc[i,housing_price_cols[h]]):\n",
    "                    data_ml_noFlood.loc[i,housing_price_cols[h]] = 0.5*(data_ml_noFlood.loc[i,housing_price_cols[h+1]] + data_ml_noFlood.loc[i,housing_price_cols[h-1]])\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml_noFlood['GDP'] = 0\n",
    "\n",
    "# loop to obtain the gdps\n",
    "for i in data_ml_noFlood.index:\n",
    "    # make a datetime object from flood year-month\n",
    "    d = datetime.datetime(int(data_ml_noFlood.loc[i,'year']), int(data_ml_noFlood.loc[i,'month']), 1)\n",
    "    \n",
    "    # extract the first GDP that is greater than the time index of the flood\n",
    "    # will put it in the right quarter\n",
    "    data_ml_noFlood.loc[i,'GDP'] = gdp.loc[gdp['datetime'] >= d,'A191RL1Q225SBEA'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml_noFlood['pop_dens'] = np.nan\n",
    "data_ml_noFlood['med_inc'] = np.nan\n",
    "\n",
    "for i in data_ml_noFlood.index:\n",
    "    if data_ml_noFlood.loc[i,'zip'] in zips_key_vals3['zip'].values:\n",
    "        data_ml_noFlood.loc[i,'pop_dens'] = zips_key_vals3.loc[zips_key_vals3['zips'].values==data_ml_noFlood.loc[i,'zip'],'pop_dens'].values\n",
    "        data_ml_noFlood.loc[i,'med_inc'] = zips_key_vals3.loc[zips_key_vals3['zips'].values==data_ml_noFlood.loc[i,'zip'],'med_hh_inc'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining pipeline and feature union before loop testing hyper parameters\n",
    "pipe_gdp_nf = Pipeline([\n",
    "            ('cst_gdp', ColumnSelectTransformer(col_names=['GDP'])),\n",
    "            ('mrs_gdp', MoveRefScale(ref=0.0, scaler='std'))\n",
    "    ])\n",
    "\n",
    "pipe_inc_nf = Pipeline([\n",
    "            ('cst_inc', ColumnSelectTransformer(col_names=['med_inc'])),\n",
    "            ('lt_inc', LogTransformer()),\n",
    "            ('mrs_inc', MoveRefScale(ref=None,scaler='std'))\n",
    "    ])\n",
    "\n",
    "pipe_popden_nf = Pipeline([\n",
    "            ('cst_pden',  ColumnSelectTransformer(col_names=['pop_dens'])),\n",
    "            ('lt_pden', LogTransformer()),\n",
    "            ('mrs_pden', MoveRefScale(ref=None,scaler='std'))\n",
    "    ])\n",
    "\n",
    "pipe_houTS_nf = Pipeline([\n",
    "            ('cst_gdp', ColumnSelectTransformer(col_names=['h-12', 'h-11','h-10','h-09', 'h-08','h-07','h-06','h-05','h-04','h-03','h-02','h-01'])),\n",
    "            ('tsr_gdp', TimeSeriesRescaler())\n",
    "    ])\n",
    "\n",
    "# feature union\n",
    "union_nf = FeatureUnion([\n",
    "    ('gdp',pipe_gdp_nf),\n",
    "    ('inc',pipe_inc_nf),\n",
    "    ('popden',pipe_popden_nf),\n",
    "    ('houTS', pipe_houTS_nf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ml_nf = data_ml[['GDP','med_inc','pop_dens','h-12', 'h-11','h-10','h-09', 'h-08','h-07','h-06','h-05','h-04','h-03','h-02','h-01']].copy()\n",
    "y_ml_nf = data_ml[['h+00','h+01','h+02','h+03','h+04','h+05','h+06','h+07','h+08','h+09','h+10','h+11','h+12']]\n",
    "X_train_nf, X_test_nf, y_train_nf, y_test_nf = train_test_split(X_ml_nf, y_ml_nf, test_size=0.35, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing to use population density as the critical value this time\n",
    "# all weights are relative to that one\n",
    "# also decreasing the range for neighbors to 100 given previous resluts\n",
    "num_hypers_test_nf = 200\n",
    "\n",
    "np.random.seed(seed=3)\n",
    "num_neighbors_nf = np.around(np.exp(np.random.uniform(np.log(5), np.log(100), num_hypers_test_nf))).astype(int)\n",
    "np.random.seed(seed=4)\n",
    "weight_gdp_nf = np.exp(np.random.uniform(np.log(0.01), np.log(100),num_hypers_test_nf))\n",
    "np.random.seed(seed=5)\n",
    "weight_medinc_nf = np.exp(np.random.uniform(np.log(0.01), np.log(100),num_hypers_test_nf))\n",
    "np.random.seed(seed=7)\n",
    "weight_houts_nf = np.exp(np.random.uniform(np.log(0.01), np.log(100),num_hypers_test_nf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_acc_knn_mix_nf = np.zeros((num_hypers_test_nf, 2))\n",
    "mse_acc_knn_mix_nf_flood = np.zeros((num_hypers_test_nf, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this calculation was aborted early due to time constraints\n",
    "# ran about 40 cases\n",
    "%%time\n",
    "\n",
    "for i in range(num_hypers_test_nf):\n",
    "    \n",
    "    hyper_loop = [num_neighbors_nf[i], weight_gdp_nf[i], weight_medinc_nf[i], 1.0, weight_houts_nf[i]]\n",
    "    \n",
    "    # set up pipeline from feature union with knn mixed estimator\n",
    "    knn_pipe_nf = Pipeline([\n",
    "                    ('union_feature', union_nf),\n",
    "                    ('mix_knn', KNNMixedTSConsts(neighbors=hyper_loop[0], ts_inds = np.arange(3,15), weights = hyper_loop[1:len(hyper_loop)]))\n",
    "        ])\n",
    "    \n",
    "    # fitting the pipeline\n",
    "    knn_pipe_nf.fit(X_train_nf,y_train_nf)\n",
    "    \n",
    "    # predicting results\n",
    "    # one is for predicting the general model (x data and y data)\n",
    "    # one if for predicting the flood errors explicitly from that model\n",
    "    # model is fit to the non-flood data\n",
    "    knn_mix_preds_nf = knn_pipe_nf.predict(X_test_nf)\n",
    "    knn_mix_preds_nf_flood = knn_pipe_nf.predict(X_ml_sh)\n",
    "    \n",
    "    mse_acc_knn_mix_nf[i,0], mse_acc_knn_mix_nf[i,1] = calc_mse_knn_mix(knn_mix_preds_nf, y_test_nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_acc_knn_mix_nf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
